{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Experiments on hash function"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The objective of this notebook is to reproduce some plots from the paper.\n",
    "Each section is dedicated to a specific experiment. \n",
    "Some experiments are based on external datasets, available on an external github repo. \n",
    "This data is fetched in a temporary file that is then deleted. \n",
    "\n",
    "\n",
    "- Exp 1 : for different datasets (Carbon-24, Perov-5, MP-20), a comparison of duplicates identified by each method.\n",
    "- Exp 2 : for different datasets (Carbon-24, MPTS-52), a comparison of the time required by each method to identify duplicates.\n",
    "- Exp 3 : for different datasets (Carbon-24, Perov-5, MP-20), an analysis of the sensitivity of each method in identifying duplicates\n",
    "- Exp 4 :  for the MPTraj dataset, a comparison of the ability of both methods to identify a material in different relaxation states.a comparison of the stability of hashing along the DFT relaxation trajectory of a material."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import some useful packages\n",
    "\n",
    "import os\n",
    "import sys\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "\n",
    "from pymatgen.core.structure import Structure\n",
    "from multiprocessing import Pool\n",
    "from functools import partial\n",
    "from datasets import load_dataset\n",
    "from scipy.stats import linregress\n",
    "\n",
    "\n",
    "# Add the parent directory to the path so we can import the module\n",
    "sys.path.append(\"..\")\n",
    "\n",
    "from material_hasher.similarity.utils.utils_experiments import (\n",
    "    download_and_merge_github_datasets,\n",
    "    process_all_hashes_and_save_csv,\n",
    "    compare_pairs_of_structure_with_pymatgen,\n",
    "    get_duplicates_from_hash,\n",
    "    concatenate_parquet_files_and_get_duplicates_from_pymatgen,\n",
    "    compare_duplicates,\n",
    "    process_times_with_different_shape_datasets,\n",
    "    apply_noise_to_structures_and_compare,\n",
    "    study_trajectories,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## EXP 1: Comparison of duplicates identified by each method"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Methodology:** For this experiment, we **pair the structures of each dataset** and compare all pairs using the StructureMatcher class. In parallel, we calculate the fingerprint for each structure, then **associate pairs of structures that have the same fingerprint**.\n",
    "\n",
    "This operation allows us to obtain the duplicates identified by the StructureMatcher class and those identified by the fingerprint. We compare the duplicates identified jointly by the fingerprint and StructureMatcher, and those that were identified by only one method or the other."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# some intermediary resultas are to be saved in this directory\n",
    "INTERMEDIARY_RESULTS_OUTPUT_DIR = \"Users/etiennedufayet/Desktop/\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if __name__ == \"__main__\":  # important to use the multiprocessing module\n",
    "    datasets = [\"mpts_52\", \"mp_20\", \"carbon_24\", \"perov_5\"]\n",
    "    for dataset in datasets:\n",
    "        df = download_and_merge_github_datasets(dataset)\n",
    "        compare_pairs_of_structure_with_pymatgen(df, INTERMEDIARY_RESULTS_OUTPUT_DIR)\n",
    "        process_all_hashes_and_save_csv(df, INTERMEDIARY_RESULTS_OUTPUT_DIR)\n",
    "\n",
    "        hashing_duplicates = get_duplicates_from_hash(\n",
    "            os.path.join(INTERMEDIARY_RESULTS_OUTPUT_DIR, \"processed_hash.csv\")\n",
    "        )\n",
    "        pymatgen_duplicates = (\n",
    "            concatenate_parquet_files_and_get_duplicates_from_pymatgen(\n",
    "                INTERMEDIARY_RESULTS_OUTPUT_DIR\n",
    "            )\n",
    "        )\n",
    "        common_rows, unique_to_pymatgen, unique_to_hash = compare_duplicates(\n",
    "            pymatgen_duplicates, hashing_duplicates\n",
    "        )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## EXP 2: Comparison of time required by each method to identify duplicates "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Methodology**: we assess the time needed to find all duplicates of a single structure in a dataset ($O(n)$ tests to perform), while varying the size of the dataset being searched. We re-compute the hashes at each iteration."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "import warnings\n",
    "\n",
    "warnings.filterwarnings(\"ignore\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# define a dictionary with the hash of the structures to compare\n",
    "HASH_STRING_TO_COMPARE = {\n",
    "    \"carbon_24\": \"8cdfcbf9aa301eb7f7f4ba991a64d5f4_1_C20\",\n",
    "    \"mpts_52\": \"69342d72a1261429349c62f610925a37_2_Na2Nd2S4O16\",\n",
    "}\n",
    "# define a dictionary with the cif string of the structures to compare\n",
    "CIF_STR = {\n",
    "    \"carbon_24\": \"\"\"\n",
    "           # generated using pymatgen\n",
    "            data_C\n",
    "            _symmetry_space_group_name_H-M   'P 1'\n",
    "            _cell_length_a   2.45939000\n",
    "            _cell_length_b   7.09478000\n",
    "            _cell_length_c   8.60230000\n",
    "            _cell_angle_alpha   94.18834000\n",
    "            _cell_angle_beta   89.91188000\n",
    "            _cell_angle_gamma   110.23094000\n",
    "            _symmetry_Int_Tables_number   1\n",
    "            _chemical_formula_structural   C\n",
    "            _chemical_formula_sum   C20\n",
    "            _cell_volume   140.41861319\n",
    "            _cell_formula_units_Z   20\n",
    "            loop_\n",
    "            _symmetry_equiv_pos_site_id\n",
    "            _symmetry_equiv_pos_as_xyz\n",
    "            1  'x, y, z'\n",
    "            loop_\n",
    "            _atom_site_type_symbol\n",
    "            _atom_site_label\n",
    "            _atom_site_symmetry_multiplicity\n",
    "            _atom_site_fract_x\n",
    "            _atom_site_fract_y\n",
    "            _atom_site_fract_z\n",
    "            _atom_site_occupancy\n",
    "            C  C0  1  0.09204560  0.53989085  0.57743291  1\n",
    "            C  C1  1  0.42239701  0.37215123  0.18400920  1\n",
    "            C  C2  1  0.56251037  1.01815515  0.78128290  1\n",
    "            C  C3  1  0.37658167  0.32786760  0.73563910  1\n",
    "            C  C4  1  0.48848319  0.94575392  0.94790445  1\n",
    "            C  C5  1  0.95725549  0.91232782  0.70982192  1\n",
    "            C  C6  1  0.75805194  0.70914403  0.78950706  1\n",
    "            C  C7  1  0.02087109  0.97791005  0.21683896  1\n",
    "            C  C8  1  0.46218982  -0.08034173  0.45657120  1\n",
    "            C  C9  1  0.55998858  0.51448437  0.06520510  1\n",
    "            C  C10  1  0.58380432  0.53299112  0.66951966  1\n",
    "            C  C11  1  0.78231763  0.23393305  0.75545699  1\n",
    "            C  C12  1  0.03689443  0.48266224  0.42067955  1\n",
    "            C  C13  1  1.21603030  0.17415536  0.14600494  1\n",
    "            C  C14  1  0.04402439  0.00197549  1.03682318  1\n",
    "            C  C15  1  -0.04943535  0.90779469  0.53732861  1\n",
    "            C  C16  1  0.50799002  0.45297318  0.34418090  1\n",
    "            C  C17  1  0.27019572  0.72199688  0.89112797  1\n",
    "            C  C18  1  0.47761773  0.93552029  0.29597244  1\n",
    "            C  C19  1  0.12625423  0.58015821  0.01236446  1\"\"\",\n",
    "    \"mpts_52\": \"\"\"# generated using pymatgen\n",
    "            data_NaNd(SO4)2\n",
    "            _symmetry_space_group_name_H-M   'P 1'\n",
    "            _cell_length_a   6.38013200\n",
    "            _cell_length_b   7.02654215\n",
    "            _cell_length_c   7.21977182\n",
    "            _cell_angle_alpha   99.29153400\n",
    "            _cell_angle_beta   96.24330201\n",
    "            _cell_angle_gamma   90.96091066\n",
    "            _symmetry_Int_Tables_number   1\n",
    "            _chemical_formula_structural   NaNd(SO4)2\n",
    "            _chemical_formula_sum   'Na2 Nd2 S4 O16'\n",
    "            _cell_volume   317.32877290\n",
    "            _cell_formula_units_Z   2\n",
    "            loop_\n",
    "            _symmetry_equiv_pos_site_id\n",
    "            _symmetry_equiv_pos_as_xyz\n",
    "            1  'x, y, z'\n",
    "            loop_\n",
    "            _atom_site_type_symbol\n",
    "            _atom_site_label\n",
    "            _atom_site_symmetry_multiplicity\n",
    "            _atom_site_fract_x\n",
    "            _atom_site_fract_y\n",
    "            _atom_site_fract_z\n",
    "            _atom_site_occupancy\n",
    "            Na  Na0  1  0.94419700  0.30603400  0.71227900  1\n",
    "            Na  Na1  1  0.05580300  0.69396600  0.28772100  1\n",
    "            Nd  Nd2  1  0.36307200  0.19516900  0.20489300  1\n",
    "            Nd  Nd3  1  0.63692800  0.80483100  0.79510700  1\n",
    "            S  S4  1  0.86909700  0.18233100  0.21381800  1\n",
    "            S  S5  1  0.13090300  0.81766900  0.78618200  1\n",
    "            S  S6  1  0.44117300  0.28617400  0.71518500  1\n",
    "            S  S7  1  0.55882700  0.71382600  0.28481500  1\n",
    "            O  O8  1  0.59262400  0.45556600  0.74750000  1\n",
    "            O  O9  1  0.40737600  0.54443400  0.25250000  1\n",
    "            O  O10  1  0.29363300  0.28887600  0.54037900  1\n",
    "            O  O11  1  0.70636700  0.71112400  0.45962100  1\n",
    "            O  O12  1  0.75502900  0.25666900  0.37703400  1\n",
    "            O  O13  1  0.24497100  0.74333100  0.62296600  1\n",
    "            O  O14  1  0.99961900  0.33738900  0.15707900  1\n",
    "            O  O15  1  0.00038100  0.66261100  0.84292100  1\n",
    "            O  O16  1  0.98124100  0.97089800  0.74956500  1\n",
    "            O  O17  1  0.01875900  0.02910200  0.25043500  1\n",
    "            O  O18  1  0.30169700  0.89528700  0.94270400  1\n",
    "            O  O19  1  0.69830300  0.10471300  0.05729600  1\n",
    "            O  O20  1  0.68511700  0.70670800  0.12260500  1\n",
    "            O  O21  1  0.31488300  0.29329200  0.87739500  1\n",
    "            O  O22  1  0.43383600  0.89041000  0.30930200  1\n",
    "            O  O23  1  0.56616400  0.10959000  0.69069800  1\"\"\",\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if __name__ == \"__main__\":\n",
    "    time_results_dict = {}\n",
    "    for dataset in [\"carbon_24\", \"mpts_52\"]:\n",
    "        hash_to_compare = HASH_STRING_TO_COMPARE[dataset]\n",
    "        structure_to_compare = Structure.from_str(CIF_STR[dataset], fmt=\"cif\")\n",
    "        sizes = [1, 10, 50, 100]\n",
    "        repeats = 5\n",
    "\n",
    "        df = download_and_merge_github_datasets(dataset)\n",
    "\n",
    "        time_results = process_times_with_different_shape_datasets(\n",
    "            df,\n",
    "            hash_to_compare,\n",
    "            structure_to_compare,\n",
    "            sizes,\n",
    "            repeats,\n",
    "        )\n",
    "        time_results_dict[dataset] = time_results"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Plot the results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for DATASET_NAME, time_results in time_results_dict.items():\n",
    "    time_results[\"multiple\"] = pd.Series([1, 10, 50, 100])\n",
    "    fig, ax = plt.subplots(figsize=(8, 4))\n",
    "\n",
    "    ax.errorbar(\n",
    "        time_results[\"multiple\"],\n",
    "        time_results[\"hash_mean_time\"],\n",
    "        yerr=time_results[\"hash_std_time\"],\n",
    "        fmt=\"o\",\n",
    "        label=\"Fingerprint\",\n",
    "        capsize=3,\n",
    "        capthick=1,\n",
    "        elinewidth=1,\n",
    "        color=\"orange\",\n",
    "        marker=\"x\",\n",
    "    )\n",
    "\n",
    "    ax.errorbar(\n",
    "        time_results[\"multiple\"],\n",
    "        time_results[\"pymatgen_mean_time\"],\n",
    "        yerr=time_results[\"pymatgen_std_time\"],\n",
    "        fmt=\".\",\n",
    "        label=\"StructureMatcher\",\n",
    "        capsize=3,\n",
    "        capthick=1,\n",
    "        elinewidth=1,\n",
    "        color=\"blue\",\n",
    "    )\n",
    "\n",
    "    # Ajustement des échelles\n",
    "    ax.set_xscale(\"log\")\n",
    "    ax.set_xlabel(\"Multiple of original dataset size\")\n",
    "    ax.set_ylabel(\"Time of research of duplicates (s)\")\n",
    "    ax.set_title(\n",
    "        f\"{DATASET_NAME} - Time for research of duplicates with increasing dataset size (semi-log scale)\"\n",
    "    )\n",
    "\n",
    "    log_x = np.log10(time_results[\"multiple\"])\n",
    "    slope_hash, intercept_hash, r_value_hash, p_value_hash, std_err_hash = linregress(\n",
    "        log_x, time_results[\"hash_mean_time\"]\n",
    "    )\n",
    "\n",
    "    slope_pmg, intercept_pmg, r_value_pmg, p_value_pmg, std_err_pmg = linregress(\n",
    "        log_x, time_results[\"pymatgen_mean_time\"]\n",
    "    )\n",
    "\n",
    "    ax.legend()\n",
    "    plt.tight_layout()\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## EXP3: Analysis of method sensitivity for duplicate identification"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The objective of this experiment is to **compare the sensitivity** threshold of each method, using **ground truth** as a point of comparison.\n",
    "\n",
    "To do this, 30 structures are randomly selected from a dataset. Each of these structures will serve as ground truth. The structures will be perturbed, then the perturbed structures will be compared with the initial structure.\n",
    "\n",
    "Here is how we proceed:\n",
    "\n",
    "- **Structure perturbation:** for each structure, Gaussian noise with a certain variance is added to the fractional coordinates (*task 1*), to the lattices (*task 2*), to both fractional coordinates and lattices (*task 3*).\n",
    "- **Matching with perturbation:** for each perturbed structure, we measure if this new structure is identical to its original structure (without noise). This measurement is performed using StructureMatcher and the fingerprint (**without the PMG label**). This operation is repeated 20 times for a given structure to average the effect of added noise.\n",
    "- **Average across the batch:** for each variance value, we perform the two operations above for each structure in the random batch of 30 structures. This gives us the proportion of noisy structures that are equal to their initial structure (without noise), for each variance value.\n",
    "- **Analysis for multiple variance values:** we vary the variance of the added noise, taking 1000 variance values between 0 and 0.3."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if __name__ == \"__main__\":\n",
    "    results = {}\n",
    "    for dataset in [\"mp_20\", \"carbon_24\", \"perov_5\"]:\n",
    "        df = download_and_merge_github_datasets(dataset)\n",
    "        df_apply_noise = df.sample(n=30, random_state=42)\n",
    "        std_list = np.linspace(0, 0.3, 100)\n",
    "\n",
    "        for noise_type in [\"lattice\", \"coords\", \"both\"]:\n",
    "            with Pool(os.cpu_count() - 2) as p:\n",
    "                _apply_noise_to_structures_and_compare = partial(\n",
    "                    apply_noise_to_structures_and_compare,\n",
    "                    df_apply_noise=df_apply_noise,\n",
    "                    noise_type=noise_type,\n",
    "                )  # define a partial function to pass the fixed arguments\n",
    "                data = p.map(_apply_noise_to_structures_and_compare, std_list)\n",
    "\n",
    "            if dataset not in results:\n",
    "                results[dataset] = {}\n",
    "\n",
    "            results[dataset][noise_type] = data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Plot the results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for DATASET_NAME in results.keys():\n",
    "    for NOISE_TYPE, data in results[DATASET_NAME].items():\n",
    "        std_values = [result[\"std\"] for result in data]\n",
    "        pymatgen_values = [np.mean(result[\"pymatgen\"]) for result in data]\n",
    "        hash_values = [np.mean(result[\"hash\"]) for result in data]\n",
    "        rmsd_values = [\n",
    "            np.mean([rmsd for rmsd in result[\"rmsd\"]])\n",
    "            for result in data\n",
    "            if all(result[\"rmsd\"])\n",
    "        ]\n",
    "        full_hash_values = [np.mean(result[\"full_hash\"]) for result in data]\n",
    "        plt.figure(figsize=(10, 6))\n",
    "\n",
    "        plt.scatter(\n",
    "            std_values,\n",
    "            pymatgen_values,\n",
    "            label=\"StructureMatcher\",\n",
    "            marker=\".\",\n",
    "            color=\"blue\",\n",
    "        )\n",
    "\n",
    "        plt.scatter(\n",
    "            std_values,\n",
    "            hash_values,\n",
    "            label=\"Only graph hash\",\n",
    "            marker=\"x\",\n",
    "            color=\"orange\",\n",
    "        )\n",
    "\n",
    "        plt.scatter(\n",
    "            std_values,\n",
    "            full_hash_values,\n",
    "            label=\"Full fingerprint\",\n",
    "            marker=\"1\",\n",
    "            color=\"green\",\n",
    "            linestyle=\"--\",\n",
    "        )\n",
    "\n",
    "        if DATASET_NAME == \"mp\":\n",
    "            dataset_name = \"MP-20\"\n",
    "        elif DATASET_NAME == \"carbon\":\n",
    "            dataset_name = \"Carbon-24\"\n",
    "        elif DATASET_NAME == \"perov\":\n",
    "            dataset_name = \"Perov-5\"\n",
    "\n",
    "        if NOISE_TYPE == \"coords\":\n",
    "            noise_type_name = \"coordinates\"\n",
    "        elif NOISE_TYPE == \"lattice\":\n",
    "            noise_type_name = \"lattice\"\n",
    "        elif NOISE_TYPE == \"both\":\n",
    "            noise_type_name = \"coordinates and lattice\"\n",
    "\n",
    "        plt.xlabel(\"Standard Deviation of added noise (std)\")\n",
    "        plt.ylabel(\"Proportion of noised structures equal to non-noised structure\")\n",
    "        plt.title(f\"{dataset_name} with noise on {noise_type_name}\")\n",
    "\n",
    "        plt.legend()\n",
    "        plt.grid(True)\n",
    "\n",
    "        plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## EXP 4: Ability of both methods to identify a material in different relaxation states"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The objective of this experiment is to compare the ability of StructureMatcher and the hash function to capture DFT relaxation effects. In this section, we investigate **whether either method can associate an unstable structure with its most stable structure.**\n",
    "\n",
    "The goal here is therefore to compare a structure along its relaxation trajectory to its relaxed structure.\n",
    "\n",
    "To do this, we use the MPTraj dataset, available on HuggingFace. We filter the dataset to obtain structures that have sufficient ionic steps (enough points on their trajectory). Each structure has several calculation IDs, meaning multiple calculations for relaxation. Since we want to study what happens near the relaxed state, **we select the calculation ID that has the best weighting between the total number of ionic steps and the number of ionic steps close to the relaxed state.**\n",
    "\n",
    "We select roughly 800 relaxation trajectories. For each trajectory and each relaxation state, we measure the matching between the structure at the current state and the final structure. The ratio between the considered ionic step and the ionic step of the relaxed structure **gives us a relaxation percentage for this measurement**.\n",
    "\n",
    "For each trajectory, we thus obtain a **list of booleans** indicating equality to the relaxed structure, associated with a **list of percentages** that represents the percentage on the final trajectory. By summing across all trajectories, with a step of 2%, we can obtain a plot that represents the proportion of structures equal to their relaxed structure according to both methods.\n",
    "\n",
    "Near the end of the trajectory, the structures have almost reached their convergence point. Thus, **the proportion of structures equal to their relaxed structure should be quite high**. The further up the trajectory we go, the further the structures are from their relaxed state. The idea is to determine whether StructureMatcher or the Hash function are still able to associate them with their relaxed state."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if __name__ == \"__main__\":\n",
    "    dataset = load_dataset(\"nimashoghi/mptrj\")\n",
    "    max_number_of_traj = 1000\n",
    "    results_list = study_trajectories(dataset, max_number_of_traj)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Plot the results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "NB_POINTS = 2  # the number of perciles that are merged together (kind of step size)\n",
    "expanded_data = []\n",
    "\n",
    "for d in results_list:\n",
    "    n_steps = len(d[\"ionic_step\"])\n",
    "    for i, step in enumerate(d[\"ionic_step\"]):\n",
    "        percentage = (1 - i / (n_steps - 1)) * 100 if n_steps > 1 else 0\n",
    "        expanded_data.append(\n",
    "            {\n",
    "                \"material_id\": d[\"material_id\"],\n",
    "                \"percentage\": percentage,\n",
    "                \"pymatgen_equality\": d[\"pymatgen_equality\"][i],\n",
    "                \"full_hash_equality\": d[\"full_hash_equality\"][i],\n",
    "                \"hash_equality\": d[\"hash_equality\"][i],\n",
    "            }\n",
    "        )\n",
    "df = pd.DataFrame(expanded_data)\n",
    "\n",
    "bins = np.linspace(0, 100, NB_POINTS)\n",
    "df[\"percentage_bin\"] = pd.cut(df[\"percentage\"], bins=bins, labels=bins[:-1])\n",
    "\n",
    "proportions = (\n",
    "    df.groupby(\"percentage_bin\")\n",
    "    .agg(\n",
    "        {\n",
    "            \"pymatgen_equality\": \"mean\",\n",
    "            \"hash_equality\": \"mean\",\n",
    "            \"full_hash_equality\": \"mean\",\n",
    "        }\n",
    "    )\n",
    "    .reset_index()\n",
    ")\n",
    "\n",
    "# ensure we are at ordinate 1 for 100% of the trajectory (no aggregation at this level)\n",
    "proportions = pd.concat(\n",
    "    [\n",
    "        pd.DataFrame(\n",
    "            {\n",
    "                \"percentage_bin\": [100],\n",
    "                \"pymatgen_equality\": [1.0],\n",
    "                \"hash_equality\": [1.0],\n",
    "                \"full_hash_equality\": [1.0],\n",
    "            }\n",
    "        ),\n",
    "        proportions,\n",
    "    ],\n",
    "    ignore_index=True,\n",
    ")\n",
    "\n",
    "fig, ax = plt.subplots(figsize=(10, 6))\n",
    "\n",
    "ax.scatter(\n",
    "    proportions[\"percentage_bin\"],\n",
    "    proportions[\"pymatgen_equality\"],\n",
    "    label=\"StructureMatcher\",\n",
    "    color=\"blue\",\n",
    "    marker=\".\",\n",
    "    s=60,\n",
    "    alpha=0.8,\n",
    ")\n",
    "\n",
    "# Plot points for \"Hash Equality\"\n",
    "ax.scatter(\n",
    "    proportions[\"percentage_bin\"],\n",
    "    proportions[\"hash_equality\"],\n",
    "    label=\"Only graph hash\",\n",
    "    color=\"orange\",\n",
    "    marker=\"x\",\n",
    "    s=60,\n",
    "    alpha=0.8,\n",
    ")\n",
    "\n",
    "ax.scatter(\n",
    "    proportions[\"percentage_bin\"],\n",
    "    proportions[\"full_hash_equality\"],\n",
    "    label=\"Full fingerprint\",\n",
    "    color=\"green\",\n",
    "    marker=\"1\",\n",
    "    linestyle=\"--\",\n",
    "    s=60,\n",
    "    alpha=0.8,\n",
    ")\n",
    "\n",
    "ax.invert_xaxis()  # Invert the x-axis to display 100% on the left\n",
    "ax.set_xlabel(\"Percentage of trajectory completed\")\n",
    "ax.set_xlabel(\"Pourcentage de la trajectoire complétée\")\n",
    "ax.set_ylabel(\"Proportion of materials equal to their first structure\")\n",
    "ax.set_title(\"Proportion of materials equal to their final structure over trajectory\")\n",
    "\n",
    "ax.legend()\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
